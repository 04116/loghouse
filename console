#!/usr/bin/env ruby
def reload!
  load './application.rb'
  Time.zone = Loghouse::TIME_ZONE
end

def insert
  names = %w[
    timestamp nsec
    string_fields.names string_fields.values
    number_fields.names number_fields.values
    boolean_fields.names boolean_fields.values
    null_fields.names
  ]

  s = CSV.generate do |csv|
    csv << names
    csv << [
      Time.now.utc.strftime('%Y-%m-%d %H:%M:%S'),
      rand(10_000),
      %w[a b c].to_s.gsub(/"/, "'"), %w[1 2 3].shuffle.to_s.gsub(/"/, "'"),
      %w[d e f].to_s.gsub(/"/, "'"), %w[4 5 6].shuffle.map(&:to_i),
      %w[g].to_s.gsub(/"/, "'"), [1],
      ['x'].to_s.gsub(/"/, "'"),
    ]
  end

  Clickhouse.connection.insert_rows(LoghouseQuery::LOGS_TABLE, csv: s)
end

def insert_fake_data
  res = Clickhouse.connection.select_rows(from: LoghouseQuery::LOGS_TABLE, limit: 500)
  s = CSV.generate do |csv|
    csv << res.names
    res.each do |r|
      o = []
      o << (Time.zone.now - rand(100_000).to_i.seconds).utc.strftime('%Y-%m-%d %H:%M:%S')
      o << rand(10**6).to_i
      r[2..10].each{|x| o << (x || []).to_s.gsub(/'/, "").gsub(/"/, "'") }
      csv << o
    end
  end

  Clickhouse.connection.insert_rows(LoghouseQuery::LOGS_TABLE, csv: s)
end

reload!

Pry.start
